{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydoc\n",
    "import connection_ms_sql \n",
    "### Create the Database in Micrsoft SQL SERVER and Then Create the Tables\n",
    "# Connection parameters\n",
    "server = 'NASIR'  # e.g., 'localhost\\SQLEXPRESS' or an IP address\n",
    "database = 'Maintenance'  # Connect to the database to create another database or just remove the database from the connection string\n",
    "username = 'nasir'  # SQL Server username, omit if using Windows authentication\n",
    "password = 'Cheema01'  # SQL Server password, omit if using Windows authentication\n",
    "driver  = 'ODBC Driver 17 for SQL Server'\n",
    "port = '1433'\n",
    "\n",
    "# Connection string\n",
    "# For Windows Authentication, use: \"Trusted_Connection=yes;\" and remove UID and PWD\n",
    "conn_str = f'DRIVER={driver};SERVER={server};UID={username};PWD={password};Port={port}'\n",
    "#pyodbc.connect(conn_str)\n",
    "\n",
    "# Create a new database named 'MyNewDatabase'\n",
    "database_name = 'Assignment'\n",
    "\n",
    "# Connect to SQL Server and create New Database\n",
    "try:\n",
    "    with pyodbc.connect(conn_str, autocommit=True) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(f\"CREATE DATABASE {database_name}\")\n",
    "            print(f\"Database '{database_name}' created successfully.\")\n",
    "except:\n",
    "    print(f'Database \"{database_name}\" already present')\n",
    "    \n",
    "# Connection string updated with database name after creation of database\n",
    "conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database_name};UID={username};PWD={password};Port={port}'\n",
    "\n",
    "# Data types for the training and ideal tables' columns\n",
    "default_data_type = \"FLOAT\"\n",
    "\n",
    "# Special data types for certain columns in the test table\n",
    "special_data_types = {\n",
    "    \"Delta Y (test func)\": \"FLOAT\",\n",
    "    \"No. of ideal func\": \"VARCHAR(255)\"\n",
    "}\n",
    "\n",
    "# Number of Columns in ideal.csv as num_columns is equal to 51 for ideal_column_names generation and the first Column name is 'X'\n",
    "num_columns = 51\n",
    "ideal_column_names = ['X'] + [f'Y{i} (ideal func)' for i in range(1, num_columns)]\n",
    "\n",
    "# Column names for each table\n",
    "tables = {\n",
    "    \"train_table\": ['X', 'Y1 (training func)', 'Y2 (training func)', 'Y3 (training func)', 'Y4 (training func)'],\n",
    "    \"test_table\": ['X (test func)', 'Y (test func)', 'Delta Y (test func)', 'No. of ideal func'],\n",
    "    \"ideal_table\": ideal_column_names,\n",
    "}\n",
    "\n",
    "try:\n",
    "    with pyodbc.connect(conn_str, autocommit=True) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            for table_name, columns in tables.items():\n",
    "                create_table_command = f\"CREATE TABLE {table_name} (\"\n",
    "                \n",
    "                for col in columns:\n",
    "                    # Determine the data type for the column\n",
    "                    if table_name == \"test_table\" and col in special_data_types:\n",
    "                        # Use special data type for certain test table columns\n",
    "                        col_data_type = special_data_types[col]\n",
    "                    else:\n",
    "                        # Default data type for other columns\n",
    "                        col_data_type = default_data_type\n",
    "                    \n",
    "                    create_table_command += f\"[{col}] {col_data_type}, \"\n",
    "                \n",
    "                create_table_command = create_table_command.rstrip(\", \")  # Remove trailing comma\n",
    "                create_table_command += \")\"\n",
    "            \n",
    "                # Execute the CREATE TABLE command\n",
    "                cursor.execute(create_table_command)\n",
    "                print(create_table_command)\n",
    "                print(f\"Table '{table_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data from CSV and then change the column names to match MS SQL Server Tables Created in the previous step and then insert the data into SQL Tables\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "server = 'NASIR'  # e.g., 'localhost\\SQLEXPRESS' or an IP address\n",
    "database = 'Assignment'  # Connect to the database to create another database or just remove the database from the connection string\n",
    "username = 'nasir'  # SQL Server username, omit if using Windows authentication\n",
    "password = 'Cheema01'  # SQL Server password, omit if using Windows authentication\n",
    "driver  = 'ODBC Driver 17 for SQL Server'\n",
    "port = '1433'\n",
    "\n",
    "# Read Csv files for train test and ideal data with in the same forlder\n",
    "dataset_path = r'C:\\Users\\nasir\\OneDrive\\Desktop\\IU-Course Data\\PROGRAMMING WITH PYTHON-course2-sem1\\3.WRITTEN ASSIGNMENT\\written Assignment\\Assignment'\n",
    "dataset_unicode_path = dataset_path.replace('\\\\','/')\n",
    "#print(dataset_unicode_path)\n",
    "train_dataset_filename = 'train.csv'\n",
    "test_dataset_filename = 'test.csv'\n",
    "ideal_dataset_filename = 'ideal.csv'\n",
    "\n",
    "#Create the DataFrame form the csv file using Pandas\n",
    "df_train = pd.read_csv(f'{dataset_unicode_path}/{train_dataset_filename}')\n",
    "df_ideal = pd.read_csv(f'{dataset_path}/{ideal_dataset_filename}')\n",
    "df_test = pd.read_csv(f'{dataset_path}/{test_dataset_filename}')\n",
    "\n",
    "num_columns = len(df_ideal.columns)\n",
    "#print(num_columns)\n",
    "# Defining a new list of column names\n",
    "train_column_names = ['X', 'Y1 (training func)', 'Y2 (training func)', 'Y3 (training func)', 'Y4 (training func)']\n",
    "test_column_names = ['X (test func)', 'Y (test func)', 'Delta Y (test func)', 'No. of ideal func']\n",
    "\n",
    "# Create a list of Ideal Function Coloum Names\n",
    "ideal_column_names = []\n",
    "for i in range(num_columns):\n",
    "    if i == 0:\n",
    "        ideal_column_names.append('X')\n",
    "    else:\n",
    "        ideal_column_names.append(f'Y{i} (ideal func)')\n",
    "#print(ideal_column_names)\n",
    "\n",
    "# Assigning the new column names to the DataFrame\n",
    "df_train.columns = train_column_names\n",
    "df_ideal.columns = ideal_column_names\n",
    "df_test.columns = test_column_names[:2]\n",
    "\n",
    "# Adding two new empty columns in test data frame\n",
    "df_test[test_column_names[2]] = ''\n",
    "df_test[test_column_names[3]] = ''\n",
    "\n",
    "print(df_train)\n",
    "print(df_ideal)\n",
    "print(df_test)\n",
    "\n",
    "### Insert the data into SQL Server\n",
    "# Your connection string to insert data line by line using sqlalchemy\n",
    "connection_string_alchemy = f'mssql+pyodbc://{username}:{password}@{server}/{database_name}?driver={driver.replace(\" \", \"+\")}&TrustServerCertificate=yes'\n",
    "engine = create_engine(connection_string_alchemy)\n",
    "#print(connection_string_alchemy)\n",
    "\n",
    "# Insert df_train into train_table\n",
    "df_train.to_sql('train_table', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Insert df_ideal into ideal_table\n",
    "df_ideal.to_sql('ideal_table', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Insert df_test into test_table\n",
    "df_test.to_sql('test_table', con=engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read from SQL SERVER and Create Pandas Data Frame form the tables\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create MS SQL 'engine' is already created from the previous step\n",
    "connection_string_alchemy = f'mssql+pyodbc://{username}:{password}@{server}/{database_name}?driver={driver.replace(\" \", \"+\")}&TrustServerCertificate=yes'\n",
    "engine = create_engine(connection_string_alchemy)\n",
    "# Read 'train_table' into a DataFrame\n",
    "query_train = \"SELECT * FROM train_table\"\n",
    "df_train = pd.read_sql_query(query_train, engine)\n",
    "\n",
    "# Read 'ideal_table' into a DataFrame\n",
    "query_ideal = \"SELECT * FROM ideal_table\"\n",
    "df_ideal = pd.read_sql_query(query_ideal, engine)\n",
    "\n",
    "# Read 'test_table' into a DataFrame\n",
    "query_test = \"SELECT * FROM test_table\"\n",
    "df_test = pd.read_sql_query(query_test, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_train and df_ideal are defined and loaded with data\n",
    "# Sort df_train and df_ideal by 'X' if not already sorted\n",
    "df_train = df_train.sort_values(by='X')\n",
    "df_ideal = df_ideal.sort_values(by='X')\n",
    "\n",
    "# Initialize a dictionary to hold the SSD sums for each training function comparison with ideal functions\n",
    "ssd_sums = {}  # Outer dictionary: keys are training functions, values are ssd_sums1 dictionaries\n",
    "top_four_ideal_functions = []  # Store the top ideal function for each training function\n",
    "\n",
    "for j in range(1, 5):  # Loop through each training function\n",
    "    ssd_sums1 = {}  # Reset ssd_sums for each training function\n",
    "    for i in range(1, 51):  # Loop through each ideal function\n",
    "        col_name = f'Y{i} (ideal func)'\n",
    "        # Calculate SSD for this training function against each ideal function\n",
    "        ssd = ((df_train.iloc[:, j] - df_ideal[col_name])**2).sum()\n",
    "        ssd_sums1[col_name] = ssd\n",
    "    ssd_sums[f'Y{j} (training func)'] = ssd_sums1  # Store ssd_sums for this training function\n",
    "    # Sort ssd_sums to find the ideal function with the lowest SSD for this training function and append its name directly\n",
    "    top_four_ideal_functions.append(sorted(ssd_sums1, key=ssd_sums1.get)[0])\n",
    "\n",
    "\n",
    "print(\"SSD sums for all comparisons:\", ssd_sums)\n",
    "print(\"Top ideal function for each training function:\", top_four_ideal_functions)\n",
    "#print(\"Top four ideal functions for the last training function comparison:\", top_four_ideal_functions1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming ssd_sums is structured as shown above\n",
    "\n",
    "# Extracting individual dictionaries for each training function\n",
    "ssd_sums_y1 = ssd_sums['Y1 (training func)']\n",
    "ssd_sums_y2 = ssd_sums['Y2 (training func)']\n",
    "ssd_sums_y3 = ssd_sums['Y3 (training func)']\n",
    "ssd_sums_y4 = ssd_sums['Y4 (training func)']\n",
    "\n",
    "# Example usage: Print the ideal function with the lowest SSD for Y1 (training func)\n",
    "lowest_ssd_y1 = min(ssd_sums_y1, key=ssd_sums_y1.get)\n",
    "print(f\"The ideal function with the lowest SSD for Y1 (training func) is {lowest_ssd_y1} with an SSD of {ssd_sums_y1[lowest_ssd_y1]}\")\n",
    "\n",
    "# If you need to perform operations or visualizations for each training function's SSD values,\n",
    "# you can iterate over the ssd_sums dictionary and process each inner dictionary as needed.\n",
    "for training_func, ssd_values in ssd_sums.items():\n",
    "    # You can now work with each ssd_values dictionary which corresponds to each training function\n",
    "    # For example, finding the lowest SSD value for each training function\n",
    "    lowest_ssd = min(ssd_values, key=ssd_values.get)\n",
    "    print(f\"The ideal function with the lowest SSD for {training_func} is {lowest_ssd} with an SSD of {ssd_values[lowest_ssd]}\")\n",
    "print(ssd_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ssd_sums_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create plot for ssd_sums\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import gridplot\n",
    "import numpy as np\n",
    "def plot(ssd_sums):\n",
    "    # Function to create a plot for given SSD sums\n",
    "    def create_ssd_plot(ssd_sums, title):\n",
    "        # Calculate log10 of SSD values\n",
    "        ssd_log = np.log10(list(ssd_sums.values()))\n",
    "        \n",
    "        # Find the minimum SSD value and its corresponding ideal function\n",
    "        min_ssd_value = min(ssd_log)\n",
    "        colors = ['green' if ssd == min_ssd_value else 'lightblue' for ssd in ssd_log]\n",
    "        \n",
    "        # Create a ColumnDataSource, now including a color field\n",
    "        ssd_source_log = ColumnDataSource(data=dict(\n",
    "            ideal_funcs=list(ssd_sums.keys()),\n",
    "            ssd=ssd_log,\n",
    "            colors=colors  # Add color information here\n",
    "        ))\n",
    "        \n",
    "        # Create the figure\n",
    "        p = figure(x_range=list(ssd_sums.keys()), title=title, width=700, height=350, tools=\"\")\n",
    "        \n",
    "        # Draw the bars, setting fill_color to use the colors from the source\n",
    "        p.vbar(x='ideal_funcs', top='ssd', width=0.9, source=ssd_source_log, line_color='white', fill_color='colors')\n",
    "        \n",
    "        # Configure hover tool\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [\n",
    "            (\"Ideal Function\", \"@ideal_funcs\"),\n",
    "            (\"Log10(SSD)\", \"@ssd{0,0.00}\")\n",
    "        ]\n",
    "        p.add_tools(hover)\n",
    "        \n",
    "        # Rotate x-axis labels for better visibility\n",
    "        p.xaxis.major_label_orientation = \"vertical\"\n",
    "        \n",
    "        return p\n",
    "\n",
    "\n",
    "    # Create a plot for each set of SSD sums\n",
    "    p1 = create_ssd_plot(ssd_sums['Y1 (training func)'], f'SSD for Y1 (training func)-Log Sclae & The Function selected is {min(ssd_sums[\"Y1 (training func)\"], key=ssd_sums[\"Y1 (training func)\"].get)} @Value {ssd_sums[\"Y1 (training func)\"][min(ssd_sums[\"Y1 (training func)\"], key=ssd_sums[\"Y1 (training func)\"].get)]}')\n",
    "    p2 = create_ssd_plot(ssd_sums['Y2 (training func)'], f'SSD for Y2 (training func)-Log Sclae & The Function selected is {min(ssd_sums[\"Y2 (training func)\"], key=ssd_sums[\"Y2 (training func)\"].get)} @Value {ssd_sums[\"Y2 (training func)\"][min(ssd_sums[\"Y2 (training func)\"], key=ssd_sums[\"Y2 (training func)\"].get)]}')\n",
    "    p3 = create_ssd_plot(ssd_sums['Y3 (training func)'], f'SSD for Y3 (training func)-Log Sclae & The Function selected is {min(ssd_sums[\"Y3 (training func)\"], key=ssd_sums[\"Y3 (training func)\"].get)} @Value {ssd_sums[\"Y3 (training func)\"][min(ssd_sums[\"Y3 (training func)\"], key=ssd_sums[\"Y3 (training func)\"].get)]}')\n",
    "    p4 = create_ssd_plot(ssd_sums['Y4 (training func)'], f'SSD for Y4 (training func)-Log Sclae & The Function selected is {min(ssd_sums[\"Y4 (training func)\"], key=ssd_sums[\"Y4 (training func)\"].get)} @Value {ssd_sums[\"Y4 (training func)\"][min(ssd_sums[\"Y4 (training func)\"], key=ssd_sums[\"Y4 (training func)\"].get)]}')\n",
    "\n",
    "    # Arrange the plots in a grid\n",
    "    grid = gridplot([[p1, p2], [p3, p4]])\n",
    "\n",
    "    # Show the grid\n",
    "    show(grid)\n",
    "    \n",
    "plot(ssd_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate deviations\n",
    "import numpy as np\n",
    "\n",
    "# Assuming chosen_ideal_functions is a list of column names for the chosen ideal functions from df_ideal\n",
    "# Example: ['Y1 (ideal func)', 'Y2 (ideal func)', ...]\n",
    "\n",
    "# Assuming the training function columns are named 'Y1 (training func)', 'Y2 (training func)', etc.\n",
    "training_function_columns = ['Y1 (training func)', 'Y2 (training func)', 'Y3 (training func)', 'Y4 (training func)']\n",
    "\n",
    "max_deviations = {}\n",
    "for ideal_func in top_four_ideal_functions:\n",
    "    all_deviations = []\n",
    "    for train_func in training_function_columns:\n",
    "        # Here, we calculate deviations for each training function against the current ideal function\n",
    "        deviations = np.abs(df_train[train_func] - df_ideal[ideal_func])\n",
    "        all_deviations.append(deviations)\n",
    "    \n",
    "    # Combine deviations from all training functions for the current ideal function\n",
    "    combined_deviations = np.concatenate(all_deviations)\n",
    "    max_deviations[ideal_func] = np.max(combined_deviations)\n",
    "\n",
    "# Adjust max deviations by factor sqrt(2)\n",
    "adjustment_factor = np.sqrt(2)\n",
    "adjusted_deviations = {func: deviation * adjustment_factor for func, deviation in max_deviations.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in test results calculations\n",
    "def find_best_match(x_val, y_val, chosen_functions, df_ideal, adjusted_deviations):\n",
    "    best_match = {'func': None, 'deviation': np.inf}\n",
    "    \n",
    "    for func in chosen_functions:\n",
    "        ideal_y_val = df_ideal.loc[df_ideal['X'] == x_val, func].iloc[0]\n",
    "        deviation = np.abs(ideal_y_val - y_val)\n",
    "        \n",
    "        if deviation < adjusted_deviations[func] and deviation < best_match['deviation']:\n",
    "            best_match = {'func': func, 'deviation': deviation}\n",
    "    \n",
    "    return best_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to store results\n",
    "test_results = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    match = find_best_match(row['X (test func)'], row['Y (test func)'], top_four_ideal_functions, df_ideal, adjusted_deviations)\n",
    "    test_results.append({\n",
    "        'X (test func)': row['X (test func)'],\n",
    "        'Y (test func)': row['Y (test func)'],\n",
    "        'Delta Y (test func)': match['deviation'] if match['func'] else None,\n",
    "        'No. of ideal func': match['func']\n",
    "        \n",
    "    })\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results = df_test_results.sort_values(by='X (test func)')\n",
    "\n",
    "# If you need to only include rows where an assignment was made, you can filter the DataFrame\n",
    "df_assigned = df_test_results.dropna(subset=['No. of ideal func'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scater plot for test results\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_test_results is already defined and loaded with data\n",
    "\n",
    "# Ensure the notebook output is set correctly\n",
    "output_notebook()\n",
    "\n",
    "# Generate a random color for each data point\n",
    "np.random.seed(42)  # For reproducibility\n",
    "colors = ['#' + ''.join([np.random.choice(list('0123456789ABCDEF')) for j in range(6)]) for i in range(len(df_test_results))]\n",
    "\n",
    "# Prepare the data\n",
    "source = ColumnDataSource(data={\n",
    "    'x_test': df_test_results['X (test func)'].astype(float),\n",
    "    'y_test': df_test_results['Y (test func)'].astype(float),\n",
    "    'ideal_func': df_test_results['No. of ideal func'],\n",
    "    'delta_y': df_test_results['Delta Y (test func)'],\n",
    "    'colors': colors  # Add the generated colors to the source\n",
    "})\n",
    "\n",
    "# Create the figure\n",
    "p = figure(width=1000, height=600, title=\"Test Results Scatter Plot\",\n",
    "           x_axis_label='X (test func)', y_axis_label='Y (test func)',\n",
    "           tools=\"pan,wheel_zoom,box_zoom,reset,save\")\n",
    "\n",
    "# Add a scatter renderer with dynamic coloring\n",
    "p.scatter('x_test', 'y_test', color='colors', source=source, size=20, alpha=0.6)\n",
    "\n",
    "# Add a hover tool\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"X (test func)\", \"@x_test\"),\n",
    "    (\"Y (test func)\", \"@y_test\"),\n",
    "    (\"No. of ideal func\", \"@ideal_func\"),\n",
    "    (\"Delta Y\", \"@delta_y\")\n",
    "]\n",
    "\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Show the result\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results.to_excel('test_results.xlsx', index=False)\n",
    "df_assigned.to_excel('assigned.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydoc\n",
    "import pandas as pd\n",
    "from connection_ms_sql import CreateDatabaseTable as cdt\n",
    "from read_csv_save_data_ms_sql import ReadCsv as rcsv\n",
    "from calculation import Calculations as cal\n",
    "from ploting import Plot as plt\n",
    "##Read Csv files for train test and ideal data with in the same forlder\n",
    "server = 'NASIR'  # e.g., 'localhost\\SQLEXPRESS' or an IP address\n",
    "database_name = 'Assignment'  # Connect to the database to create another database or just remove the database from the connection string\n",
    "username = 'nasir'  # SQL Server username, omit if using Windows authentication\n",
    "password = 'Cheema01'  # SQL Server password, omit if using Windows authentication\n",
    "driver  = 'ODBC Driver 17 for SQL Server'\n",
    "port = '1433'\n",
    "dataset_path = r'C:\\Users\\nasir\\OneDrive\\Desktop\\IU-Course Data\\PROGRAMMING WITH PYTHON-course2-sem1\\3.WRITTEN ASSIGNMENT\\written Assignment\\Assignment'\n",
    "file_names = ['train.csv','test.csv','ideal.csv']\n",
    "tabels_dic = {\n",
    "            \"train_table\": [('X', 'FLOAT'), ('Y1 (training func)', 'FLOAT'), ('Y2 (training func)', 'FLOAT'), ('Y3 (training func)', 'FLOAT'), ('Y4 (training func)', 'FLOAT')],\n",
    "            \"test_table\": [('X (test func)', 'FLOAT'), ('Y (test func)', 'FLOAT'), ('Delta Y (test func)', 'FLOAT'), ('No. of ideal func', 'VARCHAR(255)')],\n",
    "            \"ideal_table\": [('X', 'FLOAT')] + [(f'Y{i} (ideal func)', 'FLOAT') for i in range(1, 51)],\n",
    "        }\n",
    "file_to_table_map = {\n",
    "    'train.csv': 'train_table',\n",
    "    'test.csv': 'test_table',\n",
    "    'ideal.csv': 'ideal_table',\n",
    "}\n",
    "db_creator = cdt( server, \n",
    "                database_name, \n",
    "                username, \n",
    "                password, \n",
    "                driver, \n",
    "                port,\n",
    "                tabels_dic)\n",
    "db_copy =    rcsv( server, \n",
    "                database_name, \n",
    "                username, \n",
    "                password, \n",
    "                driver, \n",
    "                port,\n",
    "                dataset_path,\n",
    "                file_names,\n",
    "                tabels_dic,\n",
    "                file_to_table_map)\n",
    "\n",
    "\n",
    "db_creator.create_database()\n",
    "db_creator.create_tables()\n",
    "#print(db_creator.create_tables())\n",
    "db_copy.read_csv_to_sql()\n",
    "\n",
    "# Initialize the engine\n",
    "db_copy.alchemy_connection()\n",
    "\n",
    "# Now, you can use db_conn.engine to perform database operations\n",
    "engine = db_copy.engine\n",
    "\n",
    "# Example: Reading data into a pandas DataFrame\n",
    "df_train = pd.read_sql_query(\"SELECT * FROM train_table\", engine)\n",
    "df_ideal = pd.read_sql_query(\"SELECT * FROM ideal_table\", engine)\n",
    "df_test = pd.read_sql_query(\"SELECT * FROM test_table\", engine)\n",
    "\n",
    "# Example usage\n",
    "#print(df_train.head())\n",
    "#print(df_ideal.head())\n",
    "#print(df_test.head())\n",
    "\n",
    "# Create an instance of Class Calculations\n",
    "calculations = cal(df_train, df_ideal, df_test)\n",
    "\n",
    "# Calculate SSD sums and find top four ideal functions\n",
    "calculations.calculate_criteria1()\n",
    "\n",
    "# Access the ssd_sums and top_four_ideal_functions directly from the instance\n",
    "ssd_sums = calculations.get_ssd_sums()\n",
    "top_four_ideal_functions = calculations.get_top_four_ideal_functions()\n",
    "\n",
    "#print(\"SSD sums for all comparisons:\", ssd_sums)\n",
    "#print(\"Top ideal function for each training function:\", top_four_ideal_functions)\n",
    "\n",
    "# Calculate Daviations\n",
    "calculations.deviations()\n",
    "adjusted_deviations = calculations.get_adjusted_deviation()\n",
    "#Calculate Final Results base on df_test\n",
    "calculations.results()\n",
    "\n",
    "#Access Test Resutls\n",
    "test_results = calculations.get_test_results()\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results = df_test_results.sort_values(by='X (test func)')\n",
    "\n",
    "#Write the test Data to test_results table in Microsoft SQL Server\n",
    "# Initialize the engine\n",
    "#db_copy.alchemy_connection()\n",
    "\n",
    "# Now, you can use db_conn.engine to perform database operations\n",
    "engine = db_copy.engine\n",
    "table_name = 'test_results'\n",
    "        \n",
    "# Save the DataFrame to the SQL table\n",
    "df_test_results.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "print(f'Data Copied to {table_name} in SQL')\n",
    "\n",
    "# Create an instance of Class PlotSsdSums\n",
    "ssd = plt(ssd_sums,df_test_results)\n",
    "\n",
    "#Plot the DashBoard\n",
    "ssd.dashboard()\n",
    "#ssd.ssd_plot_only()\n",
    "#ssd.scatter_plot_only()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjusted_deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
